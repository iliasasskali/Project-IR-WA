{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/iliasasskali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from array import array\n",
    "import math\n",
    "import collections\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets in the dataset: 2399\n"
     ]
    }
   ],
   "source": [
    "# Read json and store the text of each tweet into a list\n",
    "def read_json(docs_path):\n",
    "    with open(docs_path) as fp:\n",
    "        lines = json.load(fp)\n",
    "\n",
    "    print(\"Total number of tweets in the dataset: {}\".format(len(tweets)))\n",
    "    return lines\n",
    "\n",
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "lines = read_json(docs_path)\n",
    "\n",
    "tweets = []\n",
    "for tweetId in lines:\n",
    "    tweets.append(lines[str(tweetId)][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "def remove_urls(line):\n",
    "    return url_pattern.sub(r'', line)\n",
    "\n",
    "def remove_emojis(line):\n",
    "    return emoji_pattern.sub(r'', line)\n",
    "\n",
    "def build_terms(line):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.encode(\"ascii\", \"ignore\") # Remove unicode characters\n",
    "    line = line.decode()\n",
    "    line = line.lower() ## Transform to lowercase\n",
    "    line = remove_emojis(line) ## Remove emojis, before tokenizing to delete emojis not separated by space with a word\n",
    "    line = remove_urls(line) ## Remove urls\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    line = [w for w in line if w not in stop_words]  ## eliminate the stopwords\n",
    "    line = [w for w in line if w[0]!='&' and w[-1]!=';'] ## Remove HTML symbol entity codes\n",
    "    line = [w.strip(string.punctuation.replace('#', '').replace('@', '')) for w in line] ## Remove punctuation except # and @\n",
    "    line = [stemmer.stem(w) for w in line if w!=''] ## perform stemming and remove empty words\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "ðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n",
      "\n",
      "Processed tweet:\n",
      "['intern', 'day', 'disast', 'risk', 'reduct', '#openwho', 'launch', 'multi-ti', 'core', 'curriculum', 'help', 'equip', 'compet', 'need', 'work', 'within', 'public', 'health', 'emerg', 'respons', 'start', 'learn', 'today', '#ready4respons']\n",
      "\n",
      "Original tweet:\n",
      "#COVID19 has shown how health emergencies and disasters affect entire communities â€“ especially those with weak health systems, and vulnerable populations like migrants, indigenous peoples, and those living in fragile humanitarian conditions. https://t.co/jpUQpnu0V1\n",
      "\n",
      "Processed tweet:\n",
      "['#covid19', 'shown', 'health', 'emerg', 'disast', 'affect', 'entir', 'commun', 'especi', 'weak', 'health', 'system', 'vulner', 'popul', 'like', 'migrant', 'indigen', 'peopl', 'live', 'fragil', 'humanitarian', 'condit']\n",
      "\n",
      "Original tweet:\n",
      "It's International Day for Disaster Risk Reduction\n",
      " \n",
      "To better respond to emergencies countries must:\n",
      "âœ… invest in health care systems\n",
      "âœ… achieve gender equity\n",
      "âœ… protect marginalised groups\n",
      "âœ… ensure ready &amp; equitable access to supplies\n",
      " \n",
      "A strong &amp; resilient health system is ðŸ”‘ https://t.co/5NALyjIymp\n",
      "\n",
      "Processed tweet:\n",
      "['intern', 'day', 'disast', 'risk', 'reduct', 'better', 'respond', 'emerg', 'countri', 'must', 'invest', 'health', 'care', 'system', 'achiev', 'gender', 'equiti', 'protect', 'marginalis', 'group', 'ensur', 'readi', 'equit', 'access', 'suppli', 'strong', 'resili', 'health', 'system']\n",
      "\n",
      "Original tweet:\n",
      "RT @WHOAFRO: Congratulations AlgeriaðŸ‡©ðŸ‡¿!\n",
      "\n",
      "#Algeria is the 16th country in #Africa to reach the milestone of fully vaccinating 10% of its popâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@whoafro', 'congratul', 'algeria', '#algeria', '16th', 'countri', '#africa', 'reach', 'mileston', 'fulli', 'vaccin', '10', 'pop']\n",
      "\n",
      "Original tweet:\n",
      "RT @opsoms: Si estÃ¡ completamente vacunado ðŸ’‰ðŸ’‰, Â¿aÃºn puede contraer COVID-19? \n",
      "\n",
      "ðŸš¨ No importa si estÃ¡ vacunado o si todavÃ­a estÃ¡ esperando, sâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@opsom', 'si', 'est', 'completament', 'vacunado', 'pued', 'contraer', 'covid-19', 'importa', 'si', 'est', 'vacunado', 'si', 'todava', 'est', 'esperando']\n",
      "\n",
      "Original tweet:\n",
      "RT @WHOSEARO: Is your #information coming from a source, not verified? Do not share any further. ðŸ”Ž\n",
      "\n",
      "#CheckBeforeYouShare ðŸ‘‰ðŸ½ https://t.co/VRâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@whosearo', '#inform', 'come', 'sourc', 'verifi', 'share', 'further', '#checkbeforeyoushar']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: We must appreciate the role private sector has played in the #COVID19 response and the development of vaccines in the shortesâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', 'must', 'appreci', 'role', 'privat', 'sector', 'play', '#covid19', 'respons', 'develop', 'vaccin', 'short']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: Humanity is failing miserably with vaccine injustice. Debating whether intellectual property rights should be waived in suchâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', 'human', 'fail', 'miser', 'vaccin', 'injustic', 'debat', 'whether', 'intellectu', 'properti', 'right', 'waiv']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: #COVID19 has had a major impact on peopleâ€™s #mentalhealth. Thanks, @DoctorJas of @WHOPhilippines, for your dedicated work toâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', '#covid19', 'major', 'impact', 'peopl', '#mentalhealth', 'thank', '@doctorja', '@whophilippin', 'dedic', 'work']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: Broad administration of booster doses is unfair, unjust &amp; immoral at a time when #healthworkers &amp; at most risk people in manyâ€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', 'broad', 'administr', 'booster', 'dose', 'unfair', 'unjust', 'immor', 'time', '#healthwork', 'risk', 'peopl', 'mani']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute to see first 10 original and processed tweets\n",
    "for tweet in tweets[:10]:\n",
    "    print(\"Original tweet:\\n\" + tweet + \"\\n\")\n",
    "    print(\"Processed tweet:\\n\" + str(build_terms(tweet)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: INDEXING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inverted index:\n",
    "def find_occurrences(term, tweet_text):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    term -- term to find \n",
    "    text -- original where to find terms\n",
    "\n",
    "    Returns:\n",
    "    list of positions where the term occurs in the text\n",
    "    \"\"\"\n",
    "\n",
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of json tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    #title_index = {}  # dictionary to map page titles to page ids\n",
    "    for tweetId in lines:  # lines contain all tweets, whith the id as key\n",
    "        tweet_text = lines[str(tweetId)][\"full_text\"]\n",
    "        terms = build_terms(tweet_text)\n",
    "        #title = line_arr[1]\n",
    "        #title_index[page_id]=title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "        ## START CODE\n",
    "                current_page_index[term][tweetId].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweetId, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 1.71 seconds\n",
      "Index results for the term 'researcher': []\n",
      "\n",
      "First 10 Index results for the term 'research': \n",
      "[['0', array('I', [2])], ['2', array('I', [0])], ['35', array('I', [1])], ['262', array('I', [0])], ['273', array('I', [0])], ['277', array('I', [0])], ['278', array('I', [1])], ['280', array('I', [1])], ['299', array('I', [0])], ['338', array('I', [0])]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index = create_index(lines)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))\n",
    "\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['researcher']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['intern'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Make a proposal of 5 queries that will be used to evaluate your search engine\n",
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[\"\"\"YOUR CODE HERE\"\"\" for posting in index[\"\"\"YOUR CODE HERE\"\"\"]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs = \"\"\"YOUR CODE HERE\"\"\"\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs\n",
    "\n",
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply a TF-IDF ranking to your results.\n",
    "def create_index_tfidf(lines, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    title_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for line in lines:\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = build_terms(''.join(line_arr[1:]))  #page_title + page_text\n",
    "        title = line_arr[1]\n",
    "        title_index[page_id] = title\n",
    "\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[\"\"\"YOUR CODE HERE\"\"\"][\"\"\"YOUR CODE HERE\"\"\"].append(\"\"\"YOUR CODE HERE\"\"\")\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[\"\"\"YOUR CODE HERE\"\"\"]=[\"\"\"YOUR CODE HERE\"\"\", array('I',[\"\"\"YOUR CODE HERE\"\"\"])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(\"\"\"YOUR CODE HERE\"\"\") ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(\"\"\"YOUR CODE HERE\"\"\")/\"\"\"YOUR CODE HERE\"\"\",4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] = \"\"\"YOUR CODE HERE\"\"\" # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(\"\"\"YOUR CODE HERE\"\"\"(float(\"\"\"YOUR CODE HERE\"\"\"/\"\"\"YOUR CODE HERE\"\"\")), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf, title_index = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=\"\"\"YOUR CODE HERE\"\"\"/\"\"\"YOUR CODE HERE\"\"\" * \"\"\"YOUR CODE HERE\"\"\" \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[\"\"\"YOUR CODE HERE\"\"\"(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[\"\"\"YOUR CODE HERE\"\"\" for posting in index[\"\"\"YOUR CODE HERE\"\"\"]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = \"\"\"YOUR CODE HERE\"\"\"\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    return ranked_docs\n",
    "\n",
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
