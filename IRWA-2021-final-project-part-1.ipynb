{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tester\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from array import array\n",
    "import math\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets in the dataset: 2399\n"
     ]
    }
   ],
   "source": [
    "# Read json and store the text of each tweet into a list\n",
    "def read_json(docs_path):\n",
    "    with open(docs_path) as fp:\n",
    "        lines = json.load(fp)\n",
    "\n",
    "    print(\"Total number of tweets in the dataset: {}\".format(len(lines)))\n",
    "    return lines\n",
    "\n",
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "lines = read_json(docs_path)\n",
    "\n",
    "tweets = []\n",
    "for tweetId in lines:\n",
    "    tweets.append(lines[str(tweetId)][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "def remove_urls(line):\n",
    "    return url_pattern.sub(r'', line)\n",
    "\n",
    "def remove_emojis(line):\n",
    "    return emoji_pattern.sub(r'', line)\n",
    "\n",
    "def build_terms(line):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.encode(\"ascii\", \"ignore\") # Remove unicode characters\n",
    "    line = line.decode()\n",
    "    line = line.lower() ## Transform to lowercase\n",
    "    line = remove_emojis(line) ## Remove emojis, before tokenizing to delete emojis not separated by space with a word\n",
    "    line = remove_urls(line) ## Remove urls\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    line = [w for w in line if w not in stop_words]  ## eliminate the stopwords\n",
    "    line = [w for w in line if w[0]!='&' and w[-1]!=';'] ## Remove HTML symbol entity codes\n",
    "    line = [w.strip(string.punctuation.replace('#', '').replace('@', '')) for w in line] ## Remove punctuation except # and @\n",
    "    line = [stemmer.stem(w) for w in line if w!=''] ## perform stemming and remove empty words\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "üëâ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n",
      "\n",
      "Processed tweet:\n",
      "['intern', 'day', 'disast', 'risk', 'reduct', '#openwho', 'launch', 'multi-ti', 'core', 'curriculum', 'help', 'equip', 'compet', 'need', 'work', 'within', 'public', 'health', 'emerg', 'respons', 'start', 'learn', 'today', '#ready4respons']\n",
      "\n",
      "Original tweet:\n",
      "#COVID19 has shown how health emergencies and disasters affect entire communities ‚Äì especially those with weak health systems, and vulnerable populations like migrants, indigenous peoples, and those living in fragile humanitarian conditions. https://t.co/jpUQpnu0V1\n",
      "\n",
      "Processed tweet:\n",
      "['#covid19', 'shown', 'health', 'emerg', 'disast', 'affect', 'entir', 'commun', 'especi', 'weak', 'health', 'system', 'vulner', 'popul', 'like', 'migrant', 'indigen', 'peopl', 'live', 'fragil', 'humanitarian', 'condit']\n",
      "\n",
      "Original tweet:\n",
      "It's International Day for Disaster Risk Reduction\n",
      " \n",
      "To better respond to emergencies countries must:\n",
      "‚úÖ invest in health care systems\n",
      "‚úÖ achieve gender equity\n",
      "‚úÖ protect marginalised groups\n",
      "‚úÖ ensure ready &amp; equitable access to supplies\n",
      " \n",
      "A strong &amp; resilient health system is üîë https://t.co/5NALyjIymp\n",
      "\n",
      "Processed tweet:\n",
      "['intern', 'day', 'disast', 'risk', 'reduct', 'better', 'respond', 'emerg', 'countri', 'must', 'invest', 'health', 'care', 'system', 'achiev', 'gender', 'equiti', 'protect', 'marginalis', 'group', 'ensur', 'readi', 'equit', 'access', 'suppli', 'strong', 'resili', 'health', 'system']\n",
      "\n",
      "Original tweet:\n",
      "RT @WHOAFRO: Congratulations Algeriaüá©üáø!\n",
      "\n",
      "#Algeria is the 16th country in #Africa to reach the milestone of fully vaccinating 10% of its pop‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@whoafro', 'congratul', 'algeria', '#algeria', '16th', 'countri', '#africa', 'reach', 'mileston', 'fulli', 'vaccin', '10', 'pop']\n",
      "\n",
      "Original tweet:\n",
      "RT @opsoms: Si est√° completamente vacunado üíâüíâ, ¬øa√∫n puede contraer COVID-19? \n",
      "\n",
      "üö® No importa si est√° vacunado o si todav√≠a est√° esperando, s‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@opsom', 'si', 'est', 'completament', 'vacunado', 'pued', 'contraer', 'covid-19', 'importa', 'si', 'est', 'vacunado', 'si', 'todava', 'est', 'esperando']\n",
      "\n",
      "Original tweet:\n",
      "RT @WHOSEARO: Is your #information coming from a source, not verified? Do not share any further. üîé\n",
      "\n",
      "#CheckBeforeYouShare üëâüèΩ https://t.co/VR‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@whosearo', '#inform', 'come', 'sourc', 'verifi', 'share', 'further', '#checkbeforeyoushar']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: We must appreciate the role private sector has played in the #COVID19 response and the development of vaccines in the shortes‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', 'must', 'appreci', 'role', 'privat', 'sector', 'play', '#covid19', 'respons', 'develop', 'vaccin', 'short']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: Humanity is failing miserably with vaccine injustice. Debating whether intellectual property rights should be waived in such‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', 'human', 'fail', 'miser', 'vaccin', 'injustic', 'debat', 'whether', 'intellectu', 'properti', 'right', 'waiv']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: #COVID19 has had a major impact on people‚Äôs #mentalhealth. Thanks, @DoctorJas of @WHOPhilippines, for your dedicated work to‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', '#covid19', 'major', 'impact', 'peopl', '#mentalhealth', 'thank', '@doctorja', '@whophilippin', 'dedic', 'work']\n",
      "\n",
      "Original tweet:\n",
      "RT @DrTedros: Broad administration of booster doses is unfair, unjust &amp; immoral at a time when #healthworkers &amp; at most risk people in many‚Ä¶\n",
      "\n",
      "Processed tweet:\n",
      "['rt', '@drtedro', 'broad', 'administr', 'booster', 'dose', 'unfair', 'unjust', 'immor', 'time', '#healthwork', 'risk', 'peopl', 'mani']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute to see first 10 original and processed tweets\n",
    "for tweet in tweets[:10]:\n",
    "    print(\"Original tweet:\\n\" + tweet + \"\\n\")\n",
    "    print(\"Processed tweet:\\n\" + str(build_terms(tweet)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: INDEXING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inverted index:\n",
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of json tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    tweet_index = {}  # dictionary to map tweet ids (starting from 0) to their info\n",
    "    for tweetId in lines:  # lines contain all tweets, whith the id as key\n",
    "        full_tweet = lines[str(tweetId)]\n",
    "        \n",
    "        tweet_id = full_tweet[\"id\"] # id \n",
    "        tweet = full_tweet[\"full_text\"] # Tweet\n",
    "        username = full_tweet[\"user\"][\"screen_name\"] # Username\n",
    "        date = full_tweet[\"created_at\"] # Date\n",
    "        hashtags = full_tweet[\"entities\"][\"hashtags\"] # Hashtags\n",
    "        likes = full_tweet[\"favorite_count\"] # Likes\n",
    "        retweets = full_tweet[\"retweet_count\"] # Retweets\n",
    "        url = f\"https://twitter.com/{username}/status/{tweet_id}\" # Url\n",
    "        terms = build_terms(tweet)\n",
    "        \n",
    "        # Store tweet info in the dictonary to retrieve it faster when searching\n",
    "        tweet_index[tweetId] = {}\n",
    "        tweet_index[tweetId][\"tweet\"] = tweet\n",
    "        tweet_index[tweetId][\"username\"] = username\n",
    "        tweet_index[tweetId][\"date\"] = date\n",
    "        tweet_index[tweetId][\"hashtags\"] = hashtags\n",
    "        tweet_index[tweetId][\"likes\"] = likes\n",
    "        tweet_index[tweetId][\"retweets\"] = retweets\n",
    "        tweet_index[tweetId][\"url\"] = url\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweetId, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, tweet_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 4.89 seconds\n",
      "Index results for the term 'researcher': []\n",
      "\n",
      "First 10 Index results for the term 'research': \n",
      "[['0', array('I', [0])], ['2', array('I', [0])], ['35', array('I', [1])], ['262', array('I', [0])], ['273', array('I', [0])], ['277', array('I', [0])], ['278', array('I', [1])], ['280', array('I', [1])], ['299', array('I', [0])], ['338', array('I', [0])]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index, tweet_index = create_index(lines)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))\n",
    "\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['international']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['intern'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with inserting a query\n",
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - tweet: {}\".format(d_id, tweet_index[d_id][\"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Sample of 3 results out of 236 for the query covid pandemic:\n",
      "\n",
      "page_id= 1154 - tweet: \"The first part of that hope was realized ‚Äì the development and approval of several safe and effective vaccines in record time has given the world real hope of bringing the [#COVID19] pandemic under control.\"-@DrTedros at #RC71AFRO\n",
      "page_id= 2267 - tweet: The #COVID19 pandemic has produced large amounts of #HealthData &amp; accelerated the trend towards digitalization in health. It has also exposed long-standing data governance issues such as intellectual property rights &amp; data sharing, storage &amp; reuse.\n",
      "\n",
      "üëâhttps://t.co/7o7jlbyzkB https://t.co/D6ELhFZJGN\n",
      "page_id= 2013 - tweet: The Emergency Committee on #COVID19 reconvenes every 3 months to evaluate the evolution of the pandemic &amp; make recommendations to WHO &amp; countries on how to respond to the Public Health Emergency of International Concern.\n",
      "\n",
      "Previous recommendations: https://t.co/cA4FCWWUqJ https://t.co/bmkZN64X50\n",
      "\n",
      "======================\n",
      "Sample of 3 results out of 78 for the query international disaster:\n",
      "\n",
      "page_id= 1673 - tweet: \"At the World Health Assembly in May, Member States agreed to hold a Special Session of the Assembly in November to consider developing a WHO convention, agreement or other type of international instrument on pandemic preparedness and response.\"-@DrMikeRyan\n",
      "page_id= 2013 - tweet: The Emergency Committee on #COVID19 reconvenes every 3 months to evaluate the evolution of the pandemic &amp; make recommendations to WHO &amp; countries on how to respond to the Public Health Emergency of International Concern.\n",
      "\n",
      "Previous recommendations: https://t.co/cA4FCWWUqJ https://t.co/bmkZN64X50\n",
      "page_id= 1041 - tweet: It's International #OverdoseAwarenessDay\n",
      " \n",
      "#Opioid dependence is not a self-acquired bad habit but a complex #MentalHealth condition.\n",
      " \n",
      "üëâ https://t.co/ZjGu7pgRNo https://t.co/3S6XgUZzPz\n",
      "\n",
      "======================\n",
      "Sample of 3 results out of 16 for the query ritmo de vacunacion:\n",
      "\n",
      "page_id= 824 - tweet: RT @OPSOMS_Col: El #suicidio sigue siendo un importante problema de salud p√∫blica üòî, tanto a nivel mundial como en las üåé. La #prevenci√≥ndel‚Ä¶\n",
      "page_id= 825 - tweet: RT @OPSGuate: üìçParticipa en el seminario web üíª \"Crear esperanza a trav√©s de la acci√≥n\" como parte de las actividades por el D√≠a Mundial de‚Ä¶\n",
      "page_id= 879 - tweet: RT @OMS_Afrique: Une flamb√©e de #Meningite a √©t√© d√©clar√©e dans la province de la Tshopo en #RDCüá®üá©. 261 cas suspects &amp; 129 d√©c√®s ont √©t√© sig‚Ä¶\n",
      "\n",
      "======================\n",
      "Sample of 3 results out of 18 for the query percentage de hospitalizados:\n",
      "\n",
      "page_id= 824 - tweet: RT @OPSOMS_Col: El #suicidio sigue siendo un importante problema de salud p√∫blica üòî, tanto a nivel mundial como en las üåé. La #prevenci√≥ndel‚Ä¶\n",
      "page_id= 825 - tweet: RT @OPSGuate: üìçParticipa en el seminario web üíª \"Crear esperanza a trav√©s de la acci√≥n\" como parte de las actividades por el D√≠a Mundial de‚Ä¶\n",
      "page_id= 879 - tweet: RT @OMS_Afrique: Une flamb√©e de #Meningite a √©t√© d√©clar√©e dans la province de la Tshopo en #RDCüá®üá©. 261 cas suspects &amp; 129 d√©c√®s ont √©t√© sig‚Ä¶\n",
      "\n",
      "======================\n",
      "Sample of 3 results out of 607 for the query mental health:\n",
      "\n",
      "page_id= 293 - tweet: Investment in universal health coverage has far-reaching impacts beyond health. It can also help us create stronger, more sustainable and more equitable economies and societies that leave no one behind. \n",
      "\n",
      "#HealthForAll #UNGA https://t.co/Z6OPrOFOoR\n",
      "page_id= 1493 - tweet: RT @DrTedros: .@WHO &amp; partners are:\n",
      "1. providing access to life-saving health services for those worst-affected by the crisis\n",
      "2. supporting‚Ä¶\n",
      "page_id= 2267 - tweet: The #COVID19 pandemic has produced large amounts of #HealthData &amp; accelerated the trend towards digitalization in health. It has also exposed long-standing data governance issues such as intellectual property rights &amp; data sharing, storage &amp; reuse.\n",
      "\n",
      "üëâhttps://t.co/7o7jlbyzkB https://t.co/D6ELhFZJGN\n"
     ]
    }
   ],
   "source": [
    "# 2. Make a proposal of 5 queries that will be used to evaluate your search engine\n",
    "# TODO:\n",
    "queries = [\n",
    "    \"covid pandemic\",\n",
    "    \"international disaster\",\n",
    "    \"ritmo de vacunacion\",\n",
    "    \"percentage de hospitalizados\",\n",
    "    \"mental health\"\n",
    "]\n",
    "top = 3 # Number of tweets to show\n",
    "for query in queries:\n",
    "    docs = search(query, index) # obtain the tweets resulting of such query\n",
    "    print(\"\\n======================\\nSample of {} results out of {} for the query {}:\\n\".format(top, len(docs), query))\n",
    "    for d_id in docs[:top]:\n",
    "        print(\"page_id= {} - tweet: {}\".format(d_id, tweet_index[d_id][\"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply a TF-IDF ranking to your results.\n",
    "def create_index_tfidf(lines, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets\n",
    "    num_documents -- total number of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    tweet_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweetId in lines:\n",
    "        full_tweet = lines[str(tweetId)]\n",
    "        \n",
    "        tweet_id = full_tweet[\"id\"] # id \n",
    "        tweet = full_tweet[\"full_text\"] # Tweet\n",
    "        username = full_tweet[\"user\"][\"screen_name\"] # Username\n",
    "        date = full_tweet[\"created_at\"] # Date\n",
    "        hashtags = full_tweet[\"entities\"][\"hashtags\"] # Hashtags\n",
    "        likes = full_tweet[\"favorite_count\"] # Likes\n",
    "        retweets = full_tweet[\"retweet_count\"] # Retweets\n",
    "        url = f\"https://twitter.com/{username}/status/{tweet_id}\" # Url\n",
    "        terms = build_terms(tweet)\n",
    "        \n",
    "        # Store tweet info in the dictonary to retrieve it faster when searching\n",
    "        tweet_index[tweetId] = {}\n",
    "        tweet_index[tweetId][\"tweet\"] = tweet\n",
    "        tweet_index[tweetId][\"username\"] = username\n",
    "        tweet_index[tweetId][\"date\"] = date\n",
    "        tweet_index[tweetId][\"hashtags\"] = hashtags\n",
    "        tweet_index[tweetId][\"likes\"] = likes\n",
    "        tweet_index[tweetId][\"retweets\"] = retweets\n",
    "        tweet_index[tweetId][\"url\"] = url\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweetId, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, tweet_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 120.11 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf, title_index = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_documents(terms, docs, index, idf, tf, tweet_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]= tf[term]/query_norm * idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (233,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10116/849892951.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Insert your query (i.e.: Computer Science):\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mranked_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_tf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10116/849892951.py\u001b[0m in \u001b[0;36msearch_tf_idf\u001b[1;34m(query, index)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mranked_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mranked_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10116/2950395015.py\u001b[0m in \u001b[0;36mrank_documents\u001b[1;34m(terms, docs, index, idf, tf, tweet_index)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# see np.dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mdoc_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurDocVec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurDocVec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mdoc_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mresult_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_scores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10116/2950395015.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# see np.dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mdoc_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurDocVec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurDocVec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mdoc_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mresult_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_scores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (233,) "
     ]
    }
   ],
   "source": [
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf, tweet_index)\n",
    "    return ranked_docs\n",
    "\n",
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, tweet_index[d_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ground truth of our dataset is composed of 5 Relevance Levels: [0.0, 1.0, 2.0, 3.0, 4.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "      <th>bin_y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.637926</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.824241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.358856</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.268338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id  doc_id  predicted_relevance  y_true  bin_y_true\n",
       "0     0       0            -0.637926     2.0           1\n",
       "1     0       1            -0.824241     1.0           0\n",
       "2     0       2            -1.358856     3.0           1\n",
       "3     0       3            -0.096755     1.0           0\n",
       "4     0       4            -1.268338     0.0           0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = pd.read_csv(\"inputs/test_predictions.csv\")\n",
    "search_results.head()\n",
    "print(\"The ground truth of our dataset is composed of {} Relevance Levels: {}\" .format(len(search_results[\"y_true\"].unique()), sorted(search_results[\"y_true\"].unique())))\n",
    "search_results[\"bin_y_true\"] = np.where(search_results[\"y_true\"] >= 2.0, 1, 0)\n",
    "search_results.head()\n",
    "search_results[\"bin_y_true\"] = search_results[\"y_true\"].apply(lambda y: 1 if y >=2 else 0)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_score, k=10):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    \n",
    "    '''    \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    relevant = sum(y_true == 1)\n",
    "    return float(relevant) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@5: 0.6\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n",
      "==> Precision@3: 0.6666666666666666\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n",
      "==> Precision@10: 0.6\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>y_true</th>\n",
       "      <th>bin_y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.705258</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>1.116369</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1.096797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1.084367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.082985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1.081464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>1.075457</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>1.063326</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.016901</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.906784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_id  doc_id  predicted_relevance  y_true  bin_y_true\n",
       "88      0      88             1.705258     2.0           1\n",
       "114     0     114             1.116369     2.0           1\n",
       "63      0      63             1.096797     1.0           0\n",
       "34      0      34             1.084367     1.0           0\n",
       "86      0      86             1.082985     3.0           1\n",
       "47      0      47             1.081464     0.0           0\n",
       "55      0      55             1.075457     2.0           1\n",
       "76      0      76             1.063326     3.0           1\n",
       "17      0      17             1.016901     2.0           1\n",
       "58      0      58             0.906784     1.0           0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for query 0\n",
    "\n",
    "current_query = 0\n",
    "current_query_res = search_results[search_results[\"q_id\"] == current_query] \n",
    "k=5\n",
    "\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "k=3\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "k=10\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, y_score, k=10):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    '''\n",
    "    gtp = np.sum(y_true == 1) \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])            \n",
    "    ## if all docs are not relevant\n",
    "    if gtp==0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            n_relevant_at_i +=1\n",
    "            prec_at_i += n_relevant_at_i/(i+1)\n",
    "    return prec_at_i/gtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5021658287937124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5021658287937125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(avg_precision_at_k(np.array(current_query_res[\"bin_y_true\"]), np.array(current_query_res[\"predicted_relevance\"]), 150))\n",
    "from sklearn.metrics import average_precision_score\n",
    "k = 150\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "average_precision_score(np.array(temp[\"bin_y_true\"]), np.array(temp[\"predicted_relevance\"][:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        q_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        y_true: actual score of the document for the query (ground truth).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @k : float\n",
    "    '''\n",
    "    avp = []\n",
    "    for q in search_res[\"q_id\"].unique(): #loop over all query id\n",
    "        curr_data = search_res[search_res[\"q_id\"] == q]  # select data for current query\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"bin_y_true\"]), np.array(curr_data[\"score\"]),k)) #append average precision for current query\n",
    "    return np.sum(avp)/len(avp) # return mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_k,avp = map_at_k(search_results, 10)\n",
    "map_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(y_true, y_score, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    '''\n",
    "\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true,order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(y_true) == 0: # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1/(np.argmax(y_true == 1)+1) # hint: to get the position of the first relevant document use \"np.argmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0,1,0,1,1])\n",
    "score = np.array([0.9, 0.5, 0.6, 0.7, 0.2])\n",
    "rr_at_k(y_true, score,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_query = 8\n",
    "current_query_res = search_results[search_results[\"q_id\"] == current_query] \n",
    "current_query_res.sort_values(\"score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(search_results[search_results['q_id'] == 8][\"bin_y_true\"])\n",
    "scores = np.array(search_results[search_results['q_id'] == 8][\"score\"])\n",
    "np.round(rr_at_k(labels, scores, 10),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = {}\n",
    "for k in [3,5,10]:\n",
    "    RRs = []\n",
    "    for q in search_results['q_id'].unique(): # loop over all query ids\n",
    "        labels = np.array(search_results[search_results['q_id'] == q][\"bin_y_true\"]) # get labels for current query\n",
    "        scores = np.array(search_results[search_results['q_id'] == q][\"score\"]) # get predicted score for current query\n",
    "        RRs.append(rr_at_k(labels, scores, k)) # append RR for current query\n",
    "    mrr[k] = np.round(float(sum(RRs)/len(RRs)),4) # Mean RR at current k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(y_true, y_score,  k=10):\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** y_true - 1 # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2) # Compute denominator\n",
    "    return np.sum(gain / discounts) #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=10):    \n",
    "    dcg_max = dcg_at_k(y_true, y_true, k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(y_true, y_score, k) / dcg_max,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = 0\n",
    "k = 10\n",
    "labels = np.array(search_results[search_results['q_id'] == q_id][\"y_true\"])\n",
    "scores = np.array(search_results[search_results['q_id'] == q_id][\"score\"])\n",
    "ndcg_k = np.round(ndcg_at_k(labels, scores, k),4)\n",
    "print(\"ndcg@{} for query with q_id={}: {}\".format(k,q_id,ndcg_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs = []\n",
    "k=10\n",
    "for q in search_results['q_id'].unique():\n",
    "    labels = np.array(search_results[search_results['q_id'] == q][\"y_true\"])\n",
    "    scores = np.array(search_results[search_results['q_id'] == q][\"score\"])\n",
    "    ndcgs.append(np.round(ndcg_at_k(labels, scores, k),4))\n",
    "\n",
    "avg_ndcg = np.round(float(sum(ndcgs)/len(ndcgs)),4)\n",
    "print(\"Average ndcg@{}: {}\".format(k,avg_ndcg))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8422209c9efeba716d0afa5c45b8d7468cd6f38f7fdf3cb842165b967294a9ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
