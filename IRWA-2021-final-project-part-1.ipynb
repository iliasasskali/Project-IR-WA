{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/iliasasskali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets in the dataset: 2399\n"
     ]
    }
   ],
   "source": [
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "tweets = []\n",
    "with open(docs_path) as fp:\n",
    "    tweetsJson = json.load(fp)\n",
    "\n",
    "for tweetId in tweetsJson:\n",
    "    tweets.append(tweetsJson[str(tweetId)][\"full_text\"])\n",
    "    \n",
    "print(\"Total number of tweets in the dataset: {}\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "def remove_urls(line):\n",
    "    return url_pattern.sub(r'', line)\n",
    "\n",
    "def remove_emojis(line):\n",
    "    return emoji_pattern.sub(r'', line)\n",
    "\n",
    "def build_terms(line):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.lower() ## Transform to lowercase\n",
    "    line = remove_emojis(line) ## Remove emojis, before tokenizing to delete emojis not separated by space with a word\n",
    "    line = remove_urls(line) ## Remove urls\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    line = [w for w in line if w not in stop_words]  ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line = [w.strip(string.punctuation.replace('#', '').replace('@', '')+\"\\u2026\") for w in line] ## Remove punctuation except # and @, \\u2026 is ... in unicode\n",
    "    line = [stemmer.stem(w) for w in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    line = list(filter(None, line)) ## Remove empty words\n",
    "    return line"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
