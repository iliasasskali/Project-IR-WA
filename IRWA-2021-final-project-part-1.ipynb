{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from array import array\n",
    "import math\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json and store the text of each tweet into a list\n",
    "def read_json(docs_path):\n",
    "    with open(docs_path) as fp:\n",
    "        lines = json.load(fp)\n",
    "\n",
    "    print(\"Total number of tweets in the dataset: {}\".format(len(lines)))\n",
    "    return lines\n",
    "\n",
    "docs_path = 'inputs/dataset_tweets_WHO.txt'\n",
    "lines = read_json(docs_path)\n",
    "\n",
    "tweets = []\n",
    "for tweetId in lines:\n",
    "    tweets.append(lines[str(tweetId)][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "def remove_urls(line):\n",
    "    return url_pattern.sub(r'', line)\n",
    "\n",
    "def remove_emojis(line):\n",
    "    return emoji_pattern.sub(r'', line)\n",
    "\n",
    "def build_terms(line):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.encode(\"ascii\", \"ignore\") # Remove unicode characters\n",
    "    line = line.decode()\n",
    "    line = line.lower() ## Transform to lowercase\n",
    "    line = remove_emojis(line) ## Remove emojis, before tokenizing to delete emojis not separated by space with a word\n",
    "    line = remove_urls(line) ## Remove urls\n",
    "    line = line.split() ## Tokenize the text to get a list of terms\n",
    "    line = [w for w in line if w not in stop_words]  ## eliminate the stopwords\n",
    "    line = [w for w in line if w[0]!='&' and w[-1]!=';'] ## Remove HTML symbol entity codes\n",
    "    line = [w.strip(string.punctuation.replace('#', '').replace('@', '')) for w in line] ## Remove punctuation except # and @\n",
    "    line = [stemmer.stem(w) for w in line if w!=''] ## perform stemming and remove empty words\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute to see first 10 original and processed tweets\n",
    "for tweet in tweets[:10]:\n",
    "    print(\"Original tweet:\\n\" + tweet + \"\\n\")\n",
    "    print(\"Processed tweet:\\n\" + str(build_terms(tweet)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: INDEXING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inverted index:\n",
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of json tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    tweet_index = {}  # dictionary to map tweet ids (starting from 0) to their info\n",
    "    for tweetId in lines:  # lines contain all tweets, whith the id as key\n",
    "        full_tweet = lines[str(tweetId)]\n",
    "        \n",
    "        tweet_id = full_tweet[\"id\"] # id \n",
    "        tweet = full_tweet[\"full_text\"] # Tweet\n",
    "        username = full_tweet[\"user\"][\"screen_name\"] # Username\n",
    "        date = full_tweet[\"created_at\"] # Date\n",
    "        hashtags = full_tweet[\"entities\"][\"hashtags\"] # Hashtags\n",
    "        likes = full_tweet[\"favorite_count\"] # Likes\n",
    "        retweets = full_tweet[\"retweet_count\"] # Retweets\n",
    "        url = f\"https://twitter.com/{username}/status/{tweet_id}\" # Url\n",
    "        terms = build_terms(tweet)\n",
    "        \n",
    "        # Store tweet info in the dictonary to retrieve it faster when searching\n",
    "        tweet_index[tweetId] = {}\n",
    "        tweet_index[tweetId][\"tweet\"] = tweet\n",
    "        tweet_index[tweetId][\"username\"] = username\n",
    "        tweet_index[tweetId][\"date\"] = date\n",
    "        tweet_index[tweetId][\"hashtags\"] = hashtags\n",
    "        tweet_index[tweetId][\"likes\"] = likes\n",
    "        tweet_index[tweetId][\"retweets\"] = retweets\n",
    "        tweet_index[tweetId][\"url\"] = url\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweetId, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, tweet_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index creation and test\n",
    "start_time = time.time()\n",
    "index, tweet_index = create_index(lines)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))\n",
    "\n",
    "print(\"Index results for the term 'researcher': {}\\n\".format(index['international']))\n",
    "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['intern'][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by inserting a query\n",
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - tweet: {}\".format(d_id, tweet_index[d_id][\"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Make a proposal of 5 queries that will be used to evaluate your search engine\n",
    "queries = [\n",
    "    \"covid pandemic\",\n",
    "    \"international disaster\",\n",
    "    \"ritmo de vacunacion\",\n",
    "    \"percentage de hospitalizados\",\n",
    "    \"mental health\"\n",
    "]\n",
    "top = 3 # Number of tweets to show\n",
    "for query in queries:\n",
    "    docs = search(query, index) # obtain the tweets resulting of such query\n",
    "    print(\"\\n======================\\nSample of {} results out of {} for the query {}:\\n\".format(top, len(docs), query))\n",
    "    for d_id in docs[:top]:\n",
    "        print(\"page_id= {} - tweet: {}\".format(d_id, tweet_index[d_id][\"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply a TF-IDF ranking to your results.\n",
    "def create_index_tfidf(lines, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of tweets\n",
    "    num_documents -- total number of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  # term frequencies of terms in documents\n",
    "    df = defaultdict(int)  # document frequencies of terms in the corpus\n",
    "    tweet_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweetId in lines:\n",
    "        full_tweet = lines[str(tweetId)]\n",
    "        \n",
    "        tweet_id = full_tweet[\"id\"] # id \n",
    "        tweet = full_tweet[\"full_text\"] # Tweet\n",
    "        username = full_tweet[\"user\"][\"screen_name\"] # Username\n",
    "        date = full_tweet[\"created_at\"] # Date\n",
    "        hashtags = full_tweet[\"entities\"][\"hashtags\"] # Hashtags\n",
    "        likes = full_tweet[\"favorite_count\"] # Likes\n",
    "        retweets = full_tweet[\"retweet_count\"] # Retweets\n",
    "        url = f\"https://twitter.com/{username}/status/{tweet_id}\" # Url\n",
    "        terms = build_terms(tweet)\n",
    "        \n",
    "        # Store tweet info in the dictonary to retrieve it faster when searching\n",
    "        tweet_index[tweetId] = {}\n",
    "        tweet_index[tweetId][\"tweet\"] = tweet\n",
    "        tweet_index[tweetId][\"username\"] = username\n",
    "        tweet_index[tweetId][\"date\"] = date\n",
    "        tweet_index[tweetId][\"hashtags\"] = hashtags\n",
    "        tweet_index[tweetId][\"likes\"] = likes\n",
    "        tweet_index[tweetId][\"retweets\"] = retweets\n",
    "        tweet_index[tweetId][\"url\"] = url\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term]=[tweetId, array('I',[position])]\n",
    "\n",
    "        # Normalize term frequencies\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        # Calculate the tf and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        # Merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, tweet_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf, title_index = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_documents(terms, docs, index, idf, tf, tweet_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "\n",
    "    query_norm = sum(query_terms_count.values())\n",
    "    for termIndex, term in enumerate(terms):  # termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):           \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    \n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "\n",
    "    return result_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf, tweet_index)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, tweet_index[d_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = pd.read_csv(\"inputs/test_predictions.csv\")\n",
    "search_results.head()\n",
    "print(\"The ground truth of our dataset is composed of {} Relevance Levels: {}\" .format(len(search_results[\"y_true\"].unique()), sorted(search_results[\"y_true\"].unique())))\n",
    "search_results[\"bin_y_true\"] = np.where(search_results[\"y_true\"] >= 2.0, 1, 0)\n",
    "search_results.head()\n",
    "search_results[\"bin_y_true\"] = search_results[\"y_true\"].apply(lambda y: 1 if y >=2 else 0)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_score, k=10):\n",
    "    '''    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "    \n",
    "    '''    \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    relevant = sum(y_true == 1)\n",
    "    return float(relevant) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for query 0\n",
    "\n",
    "current_query = 0\n",
    "current_query_res = search_results[search_results[\"q_id\"] == current_query] \n",
    "k=5\n",
    "\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "k=3\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "k=10\n",
    "print(\"==> Precision@{}: {}\\n\".format(k,\n",
    "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(y_true, y_score, k=10):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    '''\n",
    "    gtp = np.sum(y_true == 1) \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])            \n",
    "    ## if all docs are not relevant\n",
    "    if gtp==0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            n_relevant_at_i +=1\n",
    "            prec_at_i += n_relevant_at_i/(i+1)\n",
    "    return prec_at_i/gtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_precision_at_k(np.array(current_query_res[\"bin_y_true\"]), np.array(current_query_res[\"predicted_relevance\"]), 150))\n",
    "from sklearn.metrics import average_precision_score\n",
    "k = 150\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "average_precision_score(np.array(temp[\"bin_y_true\"]), np.array(temp[\"predicted_relevance\"][:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        q_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        y_true: actual score of the document for the query (ground truth).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @k : float\n",
    "    '''\n",
    "    avp = []\n",
    "    for q in search_res[\"q_id\"].unique(): #loop over all query id\n",
    "        curr_data = search_res[search_res[\"q_id\"] == q]  # select data for current query\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"bin_y_true\"]), np.array(curr_data[\"score\"]),k)) #append average precision for current query\n",
    "    return np.sum(avp)/len(avp) # return mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_k,avp = map_at_k(search_results, 10)\n",
    "map_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(y_true, y_score, k=10):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    '''\n",
    "\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true,order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(y_true) == 0: # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1/(np.argmax(y_true == 1)+1) # hint: to get the position of the first relevant document use \"np.argmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0,1,0,1,1])\n",
    "score = np.array([0.9, 0.5, 0.6, 0.7, 0.2])\n",
    "rr_at_k(y_true, score,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_query = 8\n",
    "current_query_res = search_results[search_results[\"q_id\"] == current_query] \n",
    "current_query_res.sort_values(\"score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(search_results[search_results['q_id'] == 8][\"bin_y_true\"])\n",
    "scores = np.array(search_results[search_results['q_id'] == 8][\"score\"])\n",
    "np.round(rr_at_k(labels, scores, 10),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = {}\n",
    "for k in [3,5,10]:\n",
    "    RRs = []\n",
    "    for q in search_results['q_id'].unique(): # loop over all query ids\n",
    "        labels = np.array(search_results[search_results['q_id'] == q][\"bin_y_true\"]) # get labels for current query\n",
    "        scores = np.array(search_results[search_results['q_id'] == q][\"score\"]) # get predicted score for current query\n",
    "        RRs.append(rr_at_k(labels, scores, k)) # append RR for current query\n",
    "    mrr[k] = np.round(float(sum(RRs)/len(RRs)),4) # Mean RR at current k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(y_true, y_score,  k=10):\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(y_true, order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** y_true - 1 # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2) # Compute denominator\n",
    "    return np.sum(gain / discounts) #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=10):    \n",
    "    dcg_max = dcg_at_k(y_true, y_true, k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(y_true, y_score, k) / dcg_max,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = 0\n",
    "k = 10\n",
    "labels = np.array(search_results[search_results['q_id'] == q_id][\"y_true\"])\n",
    "scores = np.array(search_results[search_results['q_id'] == q_id][\"score\"])\n",
    "ndcg_k = np.round(ndcg_at_k(labels, scores, k),4)\n",
    "print(\"ndcg@{} for query with q_id={}: {}\".format(k,q_id,ndcg_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs = []\n",
    "k=10\n",
    "for q in search_results['q_id'].unique():\n",
    "    labels = np.array(search_results[search_results['q_id'] == q][\"y_true\"])\n",
    "    scores = np.array(search_results[search_results['q_id'] == q][\"score\"])\n",
    "    ndcgs.append(np.round(ndcg_at_k(labels, scores, k),4))\n",
    "\n",
    "avg_ndcg = np.round(float(sum(ndcgs)/len(ndcgs)),4)\n",
    "print(\"Average ndcg@{}: {}\".format(k,avg_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8422209c9efeba716d0afa5c45b8d7468cd6f38f7fdf3cb842165b967294a9ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
